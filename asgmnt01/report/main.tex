\documentclass[9pt]{IEEEtran}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{url}
\usepackage{array}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{float}
\usepackage{gensymb}
\usepackage{longtable}
\usepackage{supertabular}
\usepackage{multicol}

\usepackage[utf8x]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\input{glyphtounicode}
\pdfgentounicode=1

\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor trig-gs}

% ============================================================================================

\title{\vspace{0ex}
Parallel Seam Carving: Performance Analysis}

\author{Erik Pahor, Jaka Å kerjanc\vspace{-4.0ex}}

% ============================================================================================

\begin{document}

\maketitle

\section{Introduction}

Seam carving is a content-aware image resizing algorithm that removes the least important seams (connected paths of pixels) from an image. In this project, we parallelized the seam carving algorithm using OpenMP to improve its performance on multi-core systems. We measured the execution time for different image sizes and core configurations, and computed the speed-up achieved by the parallel implementation. This report presents the results of our experiments and analyzes the performance of the parallelized algorithm.

\section{Experiments}

\subsection{Experimental Setup}
We tested the parallelized seam carving algorithm on five different image sizes: 592x480, 896x768, 1892x1200, 3712x2160, and 7552x4320. For each image size, we ran the algorithm multiple times, removing 128 seams each time, and averaged the execution time to obtain representative results. We experimented with different numbers of cores and threads, including configurations where the number of threads exceeded the number of cores, to evaluate the impact of hyper-threading.

\subsection{Sequential vs. Parallel Performance}
Table~\ref{tab:sequential} shows the average execution time for the sequential implementation of the seam carving algorithm using two different energy computation methods: \texttt{col\_diff\_grad} and \texttt{sobel}. As expected, the execution time increases with the image size, with the largest image (7552x4320) taking over 112 seconds for \texttt{col\_diff\_grad} and 312 seconds for \texttt{sobel}.

\begin{table}[h]
    \centering
    \caption{Average execution time (in seconds) for the sequential algorithm.}
    \label{tab:sequential}
    \begin{tabular}{|l|l|l|}
        \hline
        Energy Method & Image Size & Average Time (sec) \\ \hline
        sobel & 592x480 & 3.60515 \\ \hline
        sobel & 896x768 & 7.86012 \\ \hline
        sobel & 1892x1200 & 5.35351 \\ \hline
        sobel & 3712x2160 & 77.639 \\ \hline
        sobel & 7552x4320 & 312.353 \\ \hline
        \hline
        col\_diff\_grad & 592x480 & \textbf{1.43286} \\ \hline
        col\_diff\_grad & 896x768 & \textbf{3.35978} \\ \hline
        col\_diff\_grad & 1892x1200 & \textbf{2.15124} \\ \hline
        col\_diff\_grad & 3712x2160 & \textbf{28.1146} \\ \hline
        col\_diff\_grad & 7552x4320 & \textbf{112.177} \\ \hline
    \end{tabular}
\end{table}

\subsection{Parallel Performance}
Table~\ref{tab:parallel} presents the average execution time for the parallel implementation with different core and thread configurations. For smaller images (592x480), the best performance was achieved with two cores and two threads, suggesting that the overhead of parallelization outweighs the benefits for these sizes. However, for larger images (896x768, 1892x1200, 3712x2160, and 7552x4320), the parallel implementation significantly reduces the execution time, with the best performance achieved using 8 cores and threads for medium sized images and 16 cores and threads or more for large images.

\begin{table}[h]
    \centering
    \caption{Average execution time (in seconds) for the parallel algorithm with different core and thread configurations.}
    \label{tab:parallel}
    \begin{tabular}{|l|l|l|l|}
        \hline
        Cores & Threads & Image Size & Average Time (sec) \\ \hline
        2 & 1 & 592x480 & 0.7177 \\ \hline
        2 & 1 & 896x768 & 1.2982 \\ \hline
        2 & 1 & 1892x1200 & 0.7734 \\ \hline
        2 & 1 & 3712x2160 & 11.1835 \\ \hline
        2 & 1 & 7552x4320 & 47.9936 \\ \hline
        \hline
        2 & 2 & 592x480 & \textbf{0.6307} \\ \hline
        2 & 2 & 896x768 & 1.1683 \\ \hline
        2 & 2 & 1892x1200 & 0.9642 \\ \hline
        2 & 2 & 3712x2160 & 10.4110 \\ \hline
        2 & 2 & 7552x4320 & 43.7794 \\ \hline
        \hline
        4 & 4 & 592x480 & 0.7634 \\ \hline
        4 & 4 & 896x768 & 1.3829 \\ \hline
        4 & 4 & 1892x1200 & 0.7832 \\ \hline
        4 & 4 & 3712x2160 & 6.4311 \\ \hline
        4 & 4 & 7552x4320 & 25.1645 \\ \hline
        \hline
        8 & 8 & 592x480 & 0.7211 \\ \hline
        8 & 8 & 896x768 & \textbf{1.1608} \\ \hline
        8 & 8 & 1892x1200 & \textbf{0.6170} \\ \hline
        8 & 8 & 3712x2160 & 4.4833 \\ \hline
        8 & 8 & 7552x4320 & 16.4680 \\ \hline
        \hline
        16 & 16 & 592x480 & 0.9811 \\ \hline
        16 & 16 & 896x768 & 1.4717 \\ \hline
        16 & 16 & 1892x1200 & 0.6989 \\ \hline
        16 & 16 & 3712x2160 & \textbf{3.8650} \\ \hline
        16 & 16 & 7552x4320 & \textbf{14.8920} \\ \hline
    \end{tabular}
\end{table}

\subsection{Speed-Up Analysis}
The speed-up \( S = t_s / t_p \) was computed for each image size, where \( t_s \) is the sequential execution time and \( t_p \) is the parallel execution time with the optimal core/thread configuration. Figure~\ref{fig:speedup} shows the speed-up achieved for each image size. The largest speed-up was observed for the 7552x4320 image, where the parallel implementation achieved a speed-up by more than 7.5 times compared to the sequential version.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{speedup.png}
    \caption{Speed-up achieved by the parallel implementation for different image sizes.}
    \label{fig:speedup}
\end{figure}

\subsection{Failure Cases and Improvements}
For smaller images, the parallel implementation did not provide a very large speed-up due to the overhead of parallelization. Additionally, for very large images (7552x4320), the performance seemed to improve the more threads were used and did not hit the bottleneck. Future work could focus on optimizing the parallel algorithm for smaller images and reducing the overhead for large images by improving load balancing.

\section{Experiments (Continued)}

\subsection{Triangular Implementation Results}
After addressing memory management issues in smaller images, we implemented a triangular-blocked approach for cumulative energy calculation. Table~\ref{tab:triangular} shows the performance of this optimized implementation:

\begin{table}[h]
    \centering
    \caption{Average execution times (seconds) for triangular implementation}
    \label{tab:triangular}
    \begin{tabular}{|l|l|l|l|}
        \hline
        Cores & Threads & Image Size & Avg Time (sec) \\ \hline
        2 & 1 & 896x768 & 0.1866 \\ \hline
        2 & 1 & 1892x1200 & 0.1236 \\ \hline
        2 & 1 & 3712x2160 & \textbf{1.0222} \\ \hline
        2 & 1 & 7552x4320 & 3.8041 \\ \hline
        \hline
        2 & 2 & 896x768 & 0.1927 \\ \hline
        2 & 2 & 1892x1200 & 0.1303 \\ \hline
        2 & 2 & 3712x2160 & 1.3618 \\ \hline
        2 & 2 & 7552x4320 & 3.2807 \\ \hline
        \hline
        4 & 4 & 896x768 & \textbf{0.1726} \\ \hline
        4 & 4 & 1892x1200 & \textbf{0.1122} \\ \hline
        4 & 4 & 3712x2160 & 1.3045 \\ \hline
        4 & 4 & 7552x4320 & 2.9371 \\ \hline
        \hline
        8 & 8 & 896x768 & 0.2020 \\ \hline
        8 & 8 & 1892x1200 & 0.1183 \\ \hline
        8 & 8 & 3712x2160 & 1.2956 \\ \hline
        8 & 8 & 7552x4320 & \textbf{2.9152} \\ \hline
        \hline
        16 & 16 & 896x768 & 0.2533 \\ \hline
        16 & 16 & 1892x1200 & 0.1343 \\ \hline
        16 & 16 & 3712x2160 & 1.3625 \\ \hline
        16 & 16 & 7552x4320 & 3.0192 \\ \hline
    \end{tabular}
\end{table}

\subsection{Enhanced Speed-Up Analysis}
The triangular implementation shows significantly better scaling characteristics compared to the basic parallel approach. Figure~\ref{fig:speedup-triangular} demonstrates remarkable speed-up factors, particularly for large images:

\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{speedup_triangular.png}
    \caption{Speed-up achieved by triangular implementation (vs sequential)}
    \label{fig:speedup-triangular}
\end{figure}

\section{Conclusion}

The parallelized seam carving algorithm achieved significant speed-up for large images, with the best performance observed using up to 8 cores, than the gains seem to be very small. However, for smaller images, the overhead of parallelization outweighed the benefits. Hyper-threading provided limited improvements and often degraded performance due to resource contention. Future work could focus on optimizing the algorithm for smaller images and further reducing the overhead for large images.

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}